\documentclass{article}
\usepackage{amsmath,amsfonts,amssymb,bm,upquote}
\begin{document}

\section{Convolutional Nerual Networks}
	\subsection{Different layers in CNN}
		\subsubsection{Convolution Layer}
			\begin{flushleft}
				Convolution layer: uses filters as parameters, convolve filters with the image by multiplying its values element-wise with the original matrix.
			\end{flushleft}
			\begin{flushleft}
				In forward pass, we take many filters and convolve them on the input, each convolution gives a 2D matrix output, then stack them into a 3D volume. Usually every convolution layer is followed by a relu layer.
			\end{flushleft}
			\begin{flushleft}
				In backward pass, we calculate the derivations of each variables based on the following parameters.
			\end{flushleft}
				This is the formula for computing $dA$.
			\begin{center}
				$dA \mathrel{+}= \sum_{h=0}^{nH} \sum_{w=0}^{nW} W_c * dZ_{hw}$
			\end{center}
			\begin{flushleft}
				Where $W_c$ is a filter and $dZ_{hw}$ is a scalar corresponding to the gradient of the cost with respect to the output of the conv layer Z at the hth row and wth column (corresponding to the dot product taken at the ith stride left and jth stride down).	
			\end{flushleft}
	\subsection{Multi-class learning}
		For multi-class classification, we use Softmax function to output classification.
		\begin{center}
			$\sigma(\bm{z})_j = \frac{e^{z_j}}{\sum_{k=1}^{K}e^{z_k}}$
		\end{center}
		\subsubsection{Derivation}
			\begin{flushleft}
				If we use cross-entropy loss function, such as following:
			\end{flushleft}
			\begin{center}
				$L(\bm{W}) = - \frac{1}{N}\sum_{n=1}^{N}[y_n\log\hat{y}+(1-y_n)\log(1-\hat{y})]$
			\end{center}
			\begin{flushleft}
				Then the derivation should be:
			\end{flushleft}
			\begin{center}
				$\frac{\partial L}{\partial \bm{z}}= \bm{\hat{y}} - \bm{y}$
			\end{center}
		\subsubsection{Notes}
			\begin{flushleft}
				When used in back propagation in practical programs, espicially in mini-batch, don't forget to divide the derivation by the batch size.
			\end{flushleft}
			\begin{center}
				$\frac{\partial L}{\partial \bm{z}}= (\bm{\hat{y}} - \bm{y})/batch-size$
			\end{center}
\section{CNN Case studies}
	\subsection{Classic Networks}
		\subsubsection{LeNet-5}
			Two Convolution layers followed by avg-pool layers, then two fully-connected layer, use softmax as the output layer. As the network goes deep,
the height and width of the volume go down, and the number of filters (or the depth of the volumn) go up.
		\subsubsection{AlexNet}
			Same padding is used, $pad = (f - 1)/2$. AlexNet is similar to LeNet, but much bigger. And uses Relu as activate function.
		\subsubsection{VGG-16}
			All convolution layers are 3*3 filters, stride = 1, same pooling.\\
			All max-pooling layers are 2*2 filters, stride = 2.\\
			VGG simplied neural network architectures. The architecture of VGG-16 is uniform. Use max-pooling with 2*2 filters, stride = 2 makes the height and width go half every time.
	\subsection{ResNet}
		\subsubsection{Introduction}
			Use resudual blocks allow to build deep networks. Number of training error goes down with ResNet layer numbers goes up.
		\subsubsection{Intuition}
			\begin{center}
			$a^{[l+2]} = g(z^{[l+2]}+a^{[l]})$ \\
			$a^{[l+2]} = g(w^{[l+2]}*a^{[l+1]}+b^{[l+2]}+a^{[l]})$
			\end{center}
			\begin{flushleft}
				Use same padding convolve to ensure the demisons of the matrix. Or using pooling layers to adjust the demisons.
			\end{flushleft}
	\subsection{1*1 convolution}
		\begin{flushleft}
			Filters are 3D volume, and size is $1*1*N_{c_prev}$.Works as fully connected network with Relu. Network in Network. Useful to shrink the number of channels.
		\end{flushleft}
		\begin{flushleft}
			Inception layer uses 1*1 convolutions, other convolutions and pooling layers, and stack up all the outputs. Using 1*1 convolution reduces the computational cost without harming the performance.
		\end{flushleft}
	\subsection{Inception Network}
		\begin{flushleft}
			Inception module concatenates convolution and maxpooling layers output. Inception network puts inception modules together.\\
			Inception module makes network go deeper.
		\end{flushleft}
	\subsection{Pratical Advices}
		\begin{itemize}
			\item Use Open-Sourece Implementation
			\item Transfer Learning
			\item Data Augmentation
		\end{itemize}
		\subsubsection{Data Augmentation}
			\begin{itemize}
				\item Mirroring
				\item Random Cropping
				\item Rotation
				\item (PCA) Color shifting
			\end{itemize}
		\subsubsection{Deep Learning for Computer Vision}
			\begin{flushleft}
				Two sources of knowledge: Labeled data and hand engineering/network architecture.
			\end{flushleft}
			\begin{flushleft}
				Tips for doing well on benchmarks or competetions:
				\begin{itemize}
					\item Train several networks independently and average their outputs.
					\item Multi-crop at test time and average results.
					\item Use architectures of networks published in the literature.
					\item Use open sourece implementations if possible.
					\item Use pretrained models and fine-tune on your dataset.
				\end{itemize}
			\end{flushleft}

\section{Detection Algorithm}
	\subsection{Object Location}
		\begin{flushleft}
			For an image, the coordinate is set like this: the left up corner is set to $(0,0)$, and the right bottom corner is set to $(1,1)$. The center point of an bounding box is then $(b_x,b_y)$, in the coordinate of image. The bounding box height and width is $(b_h,b_w)$.
		\end{flushleft}
	\subsection{Convolutional Implementation of Sliding Windows}
		\begin{flushleft}
			Running sliding windows explicitly slide over the image contains a lot dublicate computation. Instead, treating the whole computation process of a window sliding as a convolutional layer, and compute the image convolutional, each part of output volume corresponds to a sliding window output.
		\end{flushleft}
	\subsection{YOLO Algorithm}
		\begin{flushleft}
			YOLO Algorithm sets grid on the image, and for each grid cell, it will feed the data into convolutional network, output the predict labels, and then use back propagation to train the convolutional network.
		\end{flushleft}
		\begin{flushleft}
			The center of a bounding box $(b_x, b_y)$ is relative position in a grid cell, so $b_x, b_y$ are between 0 and 1. So the output function of $b_x, b_y$ can be sigmoid function\\
			The heigth and width of a bounding box $(b_h, b_w)$ are relative length in a grid cell, but $b_h, b_w$ can be larger than 1. The output function of $b_h, b_w$ can be exponential function.
		\end{flushleft}
	\subsection{Intersection Over Union}
		\begin{flushleft}
			Intersection Over Union is a way to evaluating object localization. It calculate the intersection part of output and ground truth over the union part of output and ground truth.\\ Usually the output is considerred as "correct", if $IoU \geq 0.5$.\\
			More generally, IoU is a measure of the overlap between two bounding boxes.
		\end{flushleft}
	\subsection{Non-max Suppression}
		\begin{flushleft}
			Non-max Suppression is a way to make sure the algorithm detects the object only once.
		\end{flushleft}
		\begin{flushleft}
			Basically, Non-max Suppression will discard all bounding boxes with $P_c \leq 0.6$.\\
			Then it will do the following steps iteratively:
			\begin{enumerate}
				\item Pick the bounding with highest $P_c$, output that as prediction.
				\item Suppress those bounding boxes that $IoU \geq 0.5$ with previous bounding box.
			\end{enumerate}
		\end{flushleft}
	\subsection{Anchor Boxes}
		\begin{flushleft}
			Anchor boxes are pre-difined to avoid object overlapping.\\
			Each object in the training image is assigned to grid cell that contains objects midpoint and anchor box for the grid cell with the highest IoU.
		\end{flushleft}
\end{document}
